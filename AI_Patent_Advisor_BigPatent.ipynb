{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c56e0caf-1680-4f23-9c87-5d4c8b4ab170",
   "metadata": {},
   "source": [
    "# App: AI Patent Advisor\n",
    "### Dataset: BigPatent (https://huggingface.co/datasets/NortheasternUniversity/big_patent)\n",
    "\n",
    "### Features:\n",
    "**Simplified Prior Art Search:** Instead of complex keyword searches, users can describe their invention in natural language. A RAG system retrieves relevant patents from BigPatent based on semantic similarity and provides them as context to an LLM. A prompt like, \"Given this invention description [user input] and these similar patents [retrieved patents], summarize the most relevant prior art and potential novelty issues\" can generate a concise report. This simplifies the search process and makes it accessible to non-patent experts.\n",
    "\n",
    "**Competitive Technology Monitoring:** Users define a specific technology area or competitor. The system automatically retrieves newly published patents from BigPatent within that domain. A prompt such as, \"Summarize the key innovations disclosed in these recently published patents [retrieved patents] related to [technology area/competitor]\" provides a quick overview of competitive activity.\n",
    "\n",
    "**Patent Claim Analysis & Comparison:** Input two or more patent claims. The system retrieves relevant contextual information from the full patent text within BigPatent. A prompt like, \"Compare and contrast the scope of these patent claims [input claims] considering their full patent specifications [retrieved patent text]. Identify key differences and potential areas of overlap\" facilitates detailed claim analysis.\n",
    "\n",
    "**Automated Patent Summary Generation:** Input a full patent text. The system uses a prompt like, \"Generate a concise, non-legal summary of this patent [input patent] highlighting the key innovation and potential applications.\" This quickly generates summaries suitable for business audiences or technical teams, saving time and resources.\n",
    "\n",
    "**Patent Landscape Overview by CPC Class:** Users specify a CPC classification code. The system retrieves a sample of patents from BigPatent within that class. A prompt like, \"Based on these patents [retrieved patents] within CPC class [input code], summarize the current state of the art, key players, and emerging trends\" provides a quick overview of a specific technology domain. The structured nature of CPC codes simplifies retrieval and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76650cc3-8eaf-4a2d-9d7d-ecfe0d2c29a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9149a68d-faad-4778-912b-e9939d4a3709",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install gradio \n",
    "!pip -q install datasets\n",
    "!pip -q install transformers\n",
    "!pip -q install langchain \n",
    "!pip -q install sentence_transformers \n",
    "!pip -q install langchain-community faiss-cpu\n",
    "!pip -q install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9701e800-6717-4485-89e5-8c6aec32418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install bert_score\n",
    "!pip -q install langchain_openai\n",
    "!pip -q install tqdm\n",
    "!pip -q install pandas\n",
    "!pip -q install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe5887c8-ec06-4b75-8081-6cf4ff198a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 15:19:55.709490: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-06 15:19:56.049626: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733498396.171055   39687 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733498396.213411   39687 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-06 15:19:56.556292: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import streamlit as st\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "#from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from langchain_core.documents import Document\n",
    "from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "febc3a24-741e-47ca-9196-05739741ff93",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ADD KEYS\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \n",
    "os.environ[\"HF_TOKEN\"] = \n",
    "os.environ[\"OPENAI_API_KEY\"] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27feb74e-97a1-427c-9187-79f94f24917e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70b09383acf4c1d86c7276dfc944228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/9.71k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b53847fabe42d48600383741e1688a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "big_patent.py:   0%|          | 0.00/5.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a smaller subset of the 'g' CPC code patents for initial development (100 samples)\n",
    "dataset = load_dataset(\"big_patent\", \"g\", trust_remote_code=True, split=\"train[:300]\")\n",
    "texts = dataset[\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e6131cd-54b8-40ff-ae1a-048251c8d16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('rm -rf .cache/huggingface/hub/datasets--big_patent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f392bac2-35c4-4814-9159-a6b9a7790211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_open_LLM_response(model, tokenizer, input_prompt, max_length_input=1024, max_length_output=256):\n",
    "    \n",
    "    \n",
    "    inputs = tokenizer(input_prompt, return_tensors='pt', padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=1024)\n",
    "    print(len(inputs['input_ids'][0]))\n",
    "    prediction = model.generate(**inputs, max_length=256)\n",
    "    #with tokenizer.as_target_tokenizer():\n",
    "    return tokenizer.batch_decode(prediction)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "766b46ef-4144-4ec9-bfd8-bced806a63c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" CROSS REFERENCE TO RELATED APPLICATIONS \n",
    "       [0001]    This application claims benefit under 35 U.S.C. ยง119(a) of German Patent Application No. 10 2009 024 826.9-32, filed Jun. 13, 2009, the entire contents of which are incorporated herein by reference. \n",
    "       BACKGROUND OF THE INVENTION \n",
    "       [0002]    1. Field of the Invention \n",
    "         [0003]    The invention relates generally to a system for compensating electromagnetic interfering fields, and in particular to a system for magnetic field compensation having two sensors and a digital processor. \n",
    "         [0004]    2. Description of Related Art \n",
    "         [0005]    For compensating electromagnetic interfering fields, in particular magnetic interfering fields, feedback control systems are used in the very most cases, whereby one, or more sensors measure the amplitude of the interfering field for all three Cartesian space axes. The measuring signals of the sensors are fed to a control loop, which calculates control, or actuator signals from the measuring signals of the sensors, for devices generating magnetic fields. \n",
    "         [0006]    The magnetic field to be compensated may be the terrestrial magnetic field, or may be generated by other current-carrying devices being in the surrounding. \n",
    "         [0007]    Magnetic field compensation systems are for example used in connection with imaging systems using magnetic fields, for example in the case of scanning electron microscopes (SEM). \n",
    "         [0008]    In case of the mentioned devices for generating magnetic fields, it may be a matter of a current-carrying conductor, in the easiest case. Generally, one assumes interfering fields having far field characteristics, i.e. such fields, whose field amplitude does not essentially change within the range of 5 m. This assumption for example is true for interferences by rail vehicles. If the interfering fields are homogeneous in the range of interest, the compensation fields should be homogeneous, also. \n",
    "         [0009]    Pairs of so-called Helmholtz coils are preferably used for generating homogeneous compensation fields. At this, it is about two coils each being connected in the same direction, and having a distance to each other being equal to the half length of the edge (=coil diameter) (so-called Helmholtz condition). \n",
    "         [0010]    Furthermore, pairs of Helmholtz coils are used, whose distance to each other is equal to one length of the edge. If one pair of Helmholtz coils is used for each of the three space axes, the pairs of coils form a cube-shaped cage around the location, at which one, or more interfering fields shall be compensated. In case of such a coil arrangement, there indeed are field inhomogeneities in the interior of the cage, but these are acceptable in the most cases of application. \n",
    "         [0011]    A device for compensating magnetic fields is disclosed in U.S. Publication No 2005/019555A1 and has three coil pairs in a cage. The magnetic field to be compensated is measured and compensated, where an analog controller is used. \n",
    "         [0012]    Systems are also available, with which only one coil per space axis is used for generating the compensation field, however the compensation region, i.e. the region in which a good compensation is achieved, is considerably smaller than in the case of Helmholtz coils. \n",
    "         [0013]    Generally, one single magnetic field sensor is used for measuring the magnetic field at the place of interest. As an exception, there is a second sensor which is, however, used for diagnosis purposes. A single magnetic field sensor does not allow to detect, whether the magnetic field to be compensated is homogeneous, or inhomogeneous at the location of the object to be protected. \n",
    "         [0014]    It is a further problem when compensating electromagnetic interfering fields that it cannot be measured directly at the location at which the interfering field is to be compensated, since the object to be protected against interfering fields generally is at this location. \n",
    "         [0015]    A further problem arises, if two magnetic field compensation systems are arranged directly adjacent to one another. Then, undesired feedback effects may occur between the two systems. \n",
    "         [0016]    There are problems with the control systems in that these control systems can generally be optimized to single application. An adjustment to control tasks that are quite different, such as upon changes in the control configuration, is as a rule not possible or only in a restricted manner possible and/or is to be implemented with great difficulties. Furthermore non-linear control systems which may have a better interference field compensation than linear control systems, generally can only be implemented with high costs. When control circumstances change, the whole control circuit or the control loop would have to be newly calculated, designed and/or changed. In most cases, the direct user is not a position to do so. \n",
    "       SUMMARY OF THE INVENTION \n",
    "       [0017]    Therefore, it is an object of the invention to provide a system for compensating electromagnetic interfering fields with which system homogeneous as well as inhomogeneous magnetic fields may be compensated. \n",
    "         [0018]    It is a further object of the invention to perform a simulation of measuring electromagnetic interfering fields at the location of the object to be protected. \n",
    "         [0019]    It is a still further object of the invention to equalize potentially arising feedback effects in the case of using two magnetic field compensation systems in immediate vicinity. \n",
    "         [0020]    In detail, a system for compensating electromagnetic interfering fields is provided, which has two real triaxial magnetic field sensors, three pairs of compensation coils, and one control unit in order to protect an object against influences of an interfering field. It is preferred to design the control unit as a control processor such as a Digital Signal Processor DSP or a field programmable gate array FPGA. \n",
    "         [0021]    The six in total output signals of the two real sensors may be combined to three output signals of a virtual sensor, by means of a freely definable kind of averaging. By choosing the averaging algorithm properly, it can be achieved that the output signals of the virtual sensor represent the amplitude of the interfering field at the location of the object to be protected. \n",
    "         [0022]    The averaging takes place by means of the control system, which receives the six output signals of the two real magnetic field sensors via six inputs. \n",
    "         [0023]    For every sensor, the output signals of the two magnetic field sensors may be represented by a three-dimensional vector. These two vectors may be combined to six-dimensional vector, i.e. a 6ร1 matrix. The averaging over the output signals of the two real sensors, i.e. calculating the output signals of the virtual sensor, may be described by a matrix multiplication: \n",
    "         [0000]    \n",
    "       \n",
    "      \n",
    "       V=MยทS  \n",
    "      \n",
    "       \n",
    "         \n",
    "           \n",
    "             V: 6ร1 matrix of the output signals of the virtual sensor; \n",
    "             M: 6ร6 matrix describing the averaging over the output signals of the real sensors; and \n",
    "             S: 6ร1 matrix of the output signals of the virtual sensor. \n",
    "           \n",
    "         \n",
    "       \n",
    "     \n",
    "         [0027]    The now available output signals (=virtual input signals of the control system) of the virtual sensor are used as an input for independent control loops operating in parallel. These control loops may be broadband, selective concerning a frequency range, or selective concerning a frequency, also. The control loops have control algorithms transforming the virtual input signals V into changed signals {circumflex over (V)}. At this, {circumflex over (V)} is a 6ร1 matrix representing the in total six changed input signals of the control system. The control algorithm is described by an operator ฮฉ. There are no limitations concerning the control algorithm being used. Accordingly, the operator ฮฉ may not be a matrix so that nonlinear algorithms may also be used. Therefore, the transition to the modified signals {circumflex over (V)} is described by \n",
    "         [0000]        {circumflex over (V)} =ฮฉ( V )\n",
    " \n",
    "         [0028]    The matrix {circumflex over (V)} is multiplied by a 6ร6 matrix L, in order to obtain control signals for the six coils, i.e. \n",
    "         [0000]    \n",
    "       \n",
    "      \n",
    "       O=Lยท{circumflex over (V)} \n",
    "      \n",
    "     \n",
    "         [0000]    with:\n",
    " \n",
    "L: 6ร6 matrix for calculating the control signals O from the modified signals O=Lยท{circumflex over (V)}.\n",
    " \"\"\"\n",
    "\n",
    "#pred_text = tokenizer.batch_decode(prediction)\n",
    "#pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cc67d2c-3673-4fd5-969e-54b5420966fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-bigpatent\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-bigpatent\")\n",
    "# input_prompt = \"Summarize: \" + text\n",
    "# get_open_LLM_response(model, tokenizer, input_prompt, max_length_input=1024, max_length_output=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "547e96be-84d7-4f76-b14c-3fa5cc9ea14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[0:50]['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1f16962-e3cd-4125-80e6-a082c378c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8c7b5c2-7b1d-43f9-a382-2af73a745222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x7fd09a0f1750>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7fd09a0f3d90>, model='text-embedding-3-large', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "164a9075-7a38-4fa7-94bf-fd7eb4d01030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"rm -rf db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ab68edb-84d7-4842-ad9d-dd3c1d4d4376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectordb(texts, chunk_size=1024, chunk_overlap=50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    docs = []\n",
    "    id = 1\n",
    "    for text in tqdm(texts, desc=\"Creating chunks\"):\n",
    "        splits = text_splitter.split_text(text)\n",
    "        source = f\"patent_{id}\" \n",
    "        for chunk in splits:\n",
    "            docs.append(Document(page_content=chunk, metadata={\"source\": source}))\n",
    "        id += 1\n",
    "    \n",
    "\n",
    "    db = FAISS.from_documents(documents=docs,\n",
    "                              embedding=embeddings)\n",
    "    \n",
    "    return db\n",
    "\n",
    "#Create vectordb\n",
    "#db = create_vectordb(texts) \n",
    "#print(f\"FAISS VectorDB created with {len(db.index_to_docstore_id)} vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45dab317-200f-45f9-bb12-ce9531f1e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save index\n",
    "#db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "126e28ff-92c0-4e37-9bf2-af093b27ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load vectordb from the saved index\n",
    "vector_db = FAISS.load_local(\n",
    "    \"faiss_index\", embeddings, allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a3d01a3-02fe-47fc-af02-8f691650ed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae3393eb-4651-44ad-9695-312595e5f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qa_chain():\n",
    "    \n",
    "    retriever = vector_db.as_retriever(search_type=\"mmr\",\n",
    "      search_kwargs={\"k\": 4, \"fetch_k\":9}\n",
    "    )\n",
    "    qa_chain = RetrievalQA.from_llm(\n",
    "        llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0, max_tokens=1000),\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    # qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    #     llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0, max_tokens=1000),\n",
    "    #     retriever=retriever,\n",
    "    #     memory=memory,\n",
    "    #     return_source_documents=True\n",
    "    # )\n",
    "    \n",
    "    sys_prompt = \"Answer the question based on the context provided only. Do not add any new information.\"\n",
    "    qa_chain.combine_documents_chain.llm_chain.prompt.messages[0] = SystemMessagePromptTemplate.from_template(sys_prompt)\n",
    "    print(f\"qa chain: {qa_chain}\")\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62013fa9-6717-4631-8bff-fa7319620922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dceea47b25a480a9b5b2e655fd43ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a7b2bd7ecc4e29a86458bdd92d2e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.78 seconds, 1.28 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge': {'rouge1': Score(precision=0.5, recall=0.5555555555555556, fmeasure=0.5263157894736842),\n",
       "  'rouge2': Score(precision=0.2222222222222222, recall=0.25, fmeasure=0.23529411764705882),\n",
       "  'rougeL': Score(precision=0.5, recall=0.5555555555555556, fmeasure=0.5263157894736842)},\n",
       " 'bert_score': {'precision': 0.9074332118034363,\n",
       "  'recall': 0.9262628555297852,\n",
       "  'f1': 0.9167513251304626}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt: write python code to quantitatively compare human written summary with LLM generated summary using RougeScore and BERTScore\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "\n",
    "def compare_summaries(human_summary, llm_summary):\n",
    "    \"\"\"\n",
    "    Quantitatively compares a human-written summary with an LLM-generated summary\n",
    "    using ROUGE and BERTScore.\n",
    "\n",
    "    Args:\n",
    "        human_summary (str): The human-written summary.\n",
    "        llm_summary (str): The LLM-generated summary.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing ROUGE and BERTScore scores.\n",
    "    \"\"\"\n",
    "    # ROUGE Score\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(human_summary, llm_summary)\n",
    "\n",
    "    # BERTScore\n",
    "    P, R, F1 = score([llm_summary], [human_summary], lang=\"en\", verbose=True)\n",
    "    bert_score = {\n",
    "        \"precision\": P.mean().item(),\n",
    "        \"recall\": R.mean().item(),\n",
    "        \"f1\": F1.mean().item(),\n",
    "    }\n",
    "    return {\n",
    "        \"rouge\": rouge_scores,\n",
    "        \"bert_score\": bert_score,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "human_summary = \"This is a human-written summary of a document.\"\n",
    "llm_summary = \"This is an LLM-generated summary of the same document.\"\n",
    "\n",
    "comparison_results = compare_summaries(human_summary, llm_summary)\n",
    "comparison_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40a120bd-2d2b-4c5e-b22a-0633f60a8804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9074332118034363, 0.5263157894736842)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_results['bert_score']['precision'], comparison_results['rouge']['rouge1'].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd66f03b-35b4-4f17-a44f-365a9006f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context):\n",
    "    llm = ChatOpenAI(temperature=0.2, model_name=\"gpt-4o\", max_tokens=256) # or gpt-4 if available\n",
    "\n",
    "    prompt_template = \"\"\"Answer the query by using the context provided only.\\\n",
    "    Do not add any additional information. If the context is not relevant enough then\\\n",
    "    say Context not relevant enough. query: {query}\\n context: {context}\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"query\", \"context\"])\n",
    "    summary = llm(prompt.format(query=query, context=context))\n",
    "    return summary.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bf07bdc-9dcd-4f51-8554-e7d8b67fec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text):\n",
    "    llm = ChatOpenAI(temperature=0.2, model_name=\"gpt-4o\", max_tokens=128) # or gpt-4 if available\n",
    "\n",
    "    prompt_template = \"\"\"Summarize: {text}\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "    summary = llm(prompt.format(text=text))\n",
    "    return summary.content\n",
    "    \n",
    "def generate_and_compare_summaries(dataset, num_samples=3):\n",
    "    \"\"\"\n",
    "    Generates summaries for a given number of text descriptions using ChatOpenAI's GPT-4o,\n",
    "    compares them with the respective abstracts using BERTScore, and returns statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "    for i in tqdm(range(num_samples), desc=\"Generating and comparing summaries\"):\n",
    "        description = dataset[\"description\"][i]\n",
    "        abstract = dataset[\"abstract\"][i]\n",
    "\n",
    "      \n",
    "        llm_summary = generate_summary(description)\n",
    "        print(f\"description: {description}\")\n",
    "        print(f\"abstract: {abstract}\")\n",
    "        print(f\"llm summary: {llm_summary}\\n\\n\")\n",
    "        # Compare summaries\n",
    "        metrics = compare_summaries(abstract, llm_summary)\n",
    "        results.append({\n",
    "          \"description_id\": i,\n",
    "          \"bert_score_precision\": metrics['bert_score'][\"precision\"],\n",
    "          \"bert_score_recall\": metrics['bert_score'][\"recall\"],\n",
    "          \"bert_score_f1\": metrics['bert_score'][\"f1\"],\n",
    "          \"rouge1_precision\": metrics['rouge']['rouge1'].precision,\n",
    "          \"rouge1_recall\": metrics['rouge']['rouge1'].recall,\n",
    "          \"rouge1_f1\": metrics['rouge']['rouge1'].fmeasure\n",
    "        })\n",
    "      \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "#summary_results = generate_and_compare_summaries(dataset)\n",
    "\n",
    "#print(summary_results.describe()) # Prints statistics for precision, recall, and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aa7bb00-d7d4-45ed-bb1d-b1d08432275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_results = generate_and_compare_summaries(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a619909-8c9b-4fa2-97ef-a3ef9d46d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summarized_results(retrieval_query, generation_query, search_kwargs={\"k\": 3, \"fetch_k\":5}):\n",
    "   \n",
    "    retriever = vector_db.as_retriever(search_type=\"mmr\", search_kwargs=search_kwargs)\n",
    "    source_documents = retriever.invoke(retrieval_query)\n",
    "    \n",
    "    source_content_dict = {}\n",
    "    summarized_results = \"\"\n",
    "    source_metadata = \"\"\n",
    "    for doc in source_documents:\n",
    "        source = doc.metadata['source']\n",
    "        if source not in source_content_dict:\n",
    "            source_content_dict[source] = \"\"\n",
    "        source_content_dict[source] += doc.page_content\n",
    "\n",
    "    for source,context in source_content_dict.items():\n",
    "        summary = generate_response(query=generation_query, context=context)\n",
    "        summarized_results +=  f\"Source: {source}\\n\" + summary + \"\\n\\n\"\n",
    "        source_metadata += f\"Source: {source}\\nExcerpt: {context}\\n\\n\"\n",
    "        \n",
    "\n",
    "    return (summarized_results, source_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1992fa41-5b51-4ca9-b262-0d7aba7411de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retrieved_docs_metadata(result):\n",
    "    # Get metadata of retrieved documents\n",
    "    retrieved_docs_metadata = \"\"\n",
    "    if 'source_documents' in result:\n",
    "        for doc in result['source_documents']:\n",
    "            if hasattr(doc, 'metadata'):\n",
    "                retrieved_docs_metadata += f\"Metadata: {doc.metadata}\\nExcerpt: {doc.page_content}\\n\\n\"\n",
    "    return retrieved_docs_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c88a6ff-ca4e-4ea4-b1ef-4e1d73536023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patent_summarization(patent_text, query):\n",
    "    llm = ChatOpenAI(temperature=0.2, model_name=\"gpt-4o\", max_tokens=256)\n",
    "\n",
    "    prompt_template = \"\"\"{query}\n",
    "    \n",
    "    Patent text: {text}\"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"query\", \"text\"])\n",
    "    \n",
    "    response = llm(prompt.format(query=query, text=patent_text))\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3771aeb3-88fd-4f9c-9636-d97ede018d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_art_search(description):\n",
    "    summarized_results, source_metadata = get_summarized_results(retrieval_query=description, \n",
    "                                                                 generation_query=description)\n",
    "    return (summarized_results, source_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0aa88d21-93de-41f7-92fb-69a79031403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def competitive_monitoring(technology_area):\n",
    "    summarized_results, source_metadata = get_summarized_results(retrieval_query=technology_area, \n",
    "                                                                 generation_query=\"Analyze the following text and provide: 1. Summary\\n 2. Date of filing if available\\n 3.Organization or company name if available.\")\n",
    "    return (summarized_results, source_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e244508-f206-470b-abf9-6dc3c60249e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def claim_analysis(claim1, claim2):\n",
    "    qa_chain = get_qa_chain()\n",
    "    result = qa_chain(\n",
    "        {\n",
    "            \"query\": f\"Compare and contrast these two claims:\\nClaim 1: {claim1}\\nClaim 2: {claim2}\"\n",
    "        }\n",
    "    )\n",
    "    retrieved_docs_metadata = get_retrieved_docs_metadata(result)\n",
    "    return (result[\"result\"], retrieved_docs_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb42f885-6f36-4d2f-bf79-ac077ae37b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def landscape_overview(cpc_code):\n",
    "    summarized_results, source_metadata = get_summarized_results(retrieval_query=cpc_code, \n",
    "                                                                 generation_query=\"Summarize: \")\n",
    "    return (summarized_results, source_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d979f03a-204b-4ecf-864c-85e8608eff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Examples (replace with your actual examples if needed)\n",
    "prior_art_suggestions = [\n",
    "    \"Methods for detecting diseases\",\n",
    "    \"Techniques to perform image detection\",\n",
    "    \"Method to perform age-estimation of a face\",\n",
    "    \"Device for controlling flow of fluid in a microfluidic channel\"\n",
    "]\n",
    "\n",
    "monitoring_suggestions = [  # G-specific examples\n",
    "    \"Emerging Trends in Location-Based Services for Field Technicians\",\n",
    "    \"Competitive Landscape for Electromagnetic Interference Compensation\",\n",
    "    \"Innovations in Specimen Tracking and Management in Healthcare\"\n",
    "]\n",
    "\n",
    "claim_suggestions = [ # G-specific examples\n",
    "    [\"Claim 1: A method for measuring the speed of light using interferometry.\", \"Claim 2: A method for determining the refractive index of a material using interferometry.\"],\n",
    "    [\"Claim 1: A device for generating acoustic waves using piezoelectric materials.\",\"Claim 2: A device for generating acoustic waves using magnetostrictive materials.\"],\n",
    "    [\"Claim 1: A system for controlling the temperature of a superconducting magnet.\", \"Claim 2: A system for controlling the magnetic field strength of a superconducting magnet.\"]\n",
    "]\n",
    "\n",
    "\n",
    "summary_suggestions = [ # G-specific examples\n",
    "    [\"REFERENCE TO RELATED APPLICATIONS  \\n       [0001]     This application claims benefit of Application Ser. No. 60/481,847, filed Dec. 31, 2003, and of Application Ser. No. 60/561,754, filed Apr. 13, 2004.  \\n         [0002]     The entire contents of the aforementioned applications are herein incorporated by reference. The entire contents of all United States Patents and published and copending Applications mentioned below are also herein incorporated by reference. \\n     \\n    \\n     BACKGROUND OF THE INVENTION  \\n       [0003]     This invention relates to electro-optic displays and to methods for driving such displays. More specifically, in one aspect this invention relates to electro-optic displays with simplified backplanes, and methods for driving such displays. In another aspect, this invention relates to electro-optic displays in which multiple types of electro-optic units are used to improve the colors available from the displays. The present invention is especially, though not exclusively, intended for use in electrophoretic displays.  \\n         [0004]     Electro-optic displays comprise a layer of electro-optic material, a term which is used herein in its conventional meaning in the imaging art to refer to a material having first and second display states differing in at least one optical property, the material being changed from its first to its second display state by application of an electric field to the material. Although the optical property is typically color perceptible to the human eye, it may be another optical property, such as optical transmission, reflectance, luminescence or, in the case of displays intended for machine reading, pseudo-color in the sense of a change in reflectance of electromagnetic wavelengths outside the visible range.  \\n         [0005]     In the displays of the present invention, the electro-optic medium will typically be a solid (such displays may hereinafter for convenience be referred to as \\u201csolid electro-optic displays\\u201d), in the sense that the electro-optic medium has solid external surfaces, although the medium may, and often does, have internal liquid- or gas-filled spaces. Thus, the term \\u201csolid electro-optic displays\\u201d includes encapsulated electrophoretic displays, encapsulated liquid crystal displays, and other types of displays discussed below.\"[:1024]+\"...\", \"Analyze the following patent text and provide:\\n1. Concise summary\\n2. Key Invention\\n3. Potential commercial applications\"],\n",
    "]\n",
    "\n",
    "landscape_suggestions = [ # G-specific examples\n",
    "    \"CPC code: G01N (Measuring instruments; Measuring methods in general)\",\n",
    "    \"CPC code: G21B (Nuclear reactors; accessories or details thereof)\",\n",
    "    \"CPC code: G02B (Optical elements, systems, or apparatus)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03b35bb-3668-48de-9046-cbd7fed0bda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://51e26eaea4c50e209d.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://51e26eaea4c50e209d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# AI Patent Advisor Multi-Use-Case App\\n ## Dataset: BigPatent\")\n",
    "    \n",
    "    with gr.Tab(\"Patent Summarization\"):\n",
    "        \n",
    "        summary_inputs = [gr.Textbox(lines=5, placeholder=\"Enter patent text\"),\n",
    "                          gr.Textbox(lines=2, placeholder=\"Enter query\")]\n",
    "        summary_examples = gr.Examples(summary_suggestions, summary_inputs) # Query suggestions\n",
    "        summary_outputs = gr.Textbox(lines=3, label=\"Patent Summary\")\n",
    "        summary_iface = gr.Interface(\n",
    "            fn=patent_summarization,\n",
    "            inputs=summary_inputs,\n",
    "            outputs=summary_outputs,\n",
    "            flagging_mode='auto'\n",
    "        )\n",
    "    \n",
    "    with gr.Tab(\"Prior Art Search\"):\n",
    "        prior_art_inputs = gr.Textbox(lines=2, placeholder=\"Enter your invention description...\")\n",
    "        \n",
    "        prior_art_examples = gr.Examples(prior_art_suggestions, prior_art_inputs) # Query suggestions\n",
    "        prior_art_outputs = [gr.Textbox(lines=5, label=\"Potential Prior Art\"),\n",
    "                     gr.Textbox(label=\"Document Metadata\")\n",
    "            ]\n",
    "        prior_art_iface = gr.Interface(\n",
    "            fn=prior_art_search,\n",
    "            inputs=prior_art_inputs,\n",
    "            outputs=prior_art_outputs,\n",
    "            flagging_mode='auto'\n",
    "        )\n",
    "\n",
    "    with gr.Tab(\"Competitive Monitoring\"):\n",
    "        monitoring_inputs = gr.Textbox(placeholder=\"Enter technology area\")\n",
    "        monitoring_examples = gr.Examples(monitoring_suggestions, monitoring_inputs) # Query suggestions\n",
    "        monitoring_outputs = [gr.Textbox(lines=5, label=\"Competitive Landscape\"),\n",
    "                     gr.Textbox(label=\"Document Metadata\")\n",
    "            ]\n",
    "        monitoring_iface = gr.Interface(\n",
    "            fn=competitive_monitoring,\n",
    "            inputs=monitoring_inputs,\n",
    "            outputs=monitoring_outputs,\n",
    "            flagging_mode='auto'\n",
    "        )\n",
    "\n",
    "    with gr.Tab(\"Claims Comparison\"):\n",
    "        claim_inputs = [\n",
    "                gr.Textbox(lines=2, placeholder=\"Enter claim 1\"),\n",
    "                gr.Textbox(lines=2, placeholder=\"Enter claim 2\"),\n",
    "            ]\n",
    "        claim_examples = gr.Examples(claim_suggestions, claim_inputs) # Query suggestions\n",
    "        claim_outputs = [gr.Textbox(lines=5, label=\"Claim Comparison\"),\n",
    "                     gr.Textbox(label=\"Document Metadata\")\n",
    "            ]\n",
    "        claim_iface = gr.Interface(\n",
    "            fn=claim_analysis,\n",
    "            inputs=claim_inputs,\n",
    "            outputs=claim_outputs,\n",
    "            flagging_mode='auto'\n",
    "        )  \n",
    "        \n",
    "    with gr.Tab(\"Landscape Overview\"):\n",
    "        landscape_inputs = gr.Textbox(placeholder=\"Enter CPC code\")\n",
    "        ladscape_examples = gr.Examples(landscape_suggestions, landscape_inputs) # Query suggestions\n",
    "        landscape_outputs = [gr.Textbox(lines=5, label=\"Landscape Overview\"),\n",
    "                     gr.Textbox(label=\"Document Metadata\")\n",
    "            ]\n",
    "        landscape_iface = gr.Interface(\n",
    "            fn=landscape_overview,\n",
    "            inputs=landscape_inputs,\n",
    "            outputs=landscape_outputs,\n",
    "            flagging_mode='auto'\n",
    "        )\n",
    "        \n",
    "demo.launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69544c69-796e-4aa9-89c8-79ebc5720693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90148fca-367a-4345-b666-ba31bd15e6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
